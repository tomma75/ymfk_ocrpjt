#!/usr/bin/env python3
"""
YOKOGAWA OCR ë°ì´í„° ì¤€ë¹„ í”„ë¡œì íŠ¸ - í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€ ì´ˆê¸°í™” ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€ì˜ ì´ˆê¸°í™”ë¥¼ ë‹´ë‹¹í•˜ë©°,
ëª¨ë“  í…ŒìŠ¤íŠ¸ ëª¨ë“ˆì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í…ŒìŠ¤íŠ¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.

ì‘ì„±ì: YOKOGAWA OCR ê°œë°œíŒ€
ì‘ì„±ì¼: 2025-07-18
"""

import os
import sys
import unittest
import tempfile
import shutil
import json
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Callable
from datetime import datetime
from unittest.mock import Mock, patch, MagicMock
import uuid

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ëª¨ë“ˆ import
from config.settings import ApplicationConfig, load_configuration
from core.exceptions import *
from services.data_collection_service import create_data_collection_service
from services.labeling_service import create_labeling_service
from services.augmentation_service import create_augmentation_service
from services.validation_service import create_validation_service
from models.document_model import DocumentModel, DocumentMetadata, PageInfo
from models.annotation_model import AnnotationModel, BoundingBox, FieldAnnotation
from utils.logger_util import setup_logger, get_application_logger
from utils.file_handler import FileHandler
from utils.image_processor import ImageProcessor

# ====================================================================================
# 1. í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€ ë©”íƒ€ë°ì´í„°
# ====================================================================================

__version__ = "1.0.0"
__author__ = "YOKOGAWA OCR ê°œë°œíŒ€"
__email__ = "ocr-dev@yokogawa.com"
__description__ = "YOKOGAWA OCR ë°ì´í„° ì¤€ë¹„ í”„ë¡œì íŠ¸ - í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€"

# ====================================================================================
# 2. í…ŒìŠ¤íŠ¸ ì„¤ì • ë° ìƒìˆ˜
# ====================================================================================

# í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
TEST_ENVIRONMENT = "testing"
TEST_LOG_LEVEL = "DEBUG"
TEST_TIMEOUT_SECONDS = 30
TEST_BATCH_SIZE = 5
TEST_MAX_WORKERS = 2

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì •
TEST_DATA_DIR = "test_data"
TEST_PDF_FILE = "test_document.pdf"
TEST_IMAGE_FILE = "test_image.jpg"
TEST_JSON_FILE = "test_data.json"

# í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì„¤ì •
TEST_REPORT_DIR = "test_reports"
TEST_COVERAGE_MIN = 80.0

# ====================================================================================
# 3. í…ŒìŠ¤íŠ¸ í—¬í¼ í•¨ìˆ˜ë“¤
# ====================================================================================


def create_test_config() -> ApplicationConfig:
    """
    í…ŒìŠ¤íŠ¸ìš© ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ìƒì„±

    Returns:
        ApplicationConfig: í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ê°ì²´
    """
    config = ApplicationConfig()
    config.environment = TEST_ENVIRONMENT
    config.debug_mode = True
    config.data_directory = tempfile.mkdtemp(prefix="yokogawa_test_")
    config.raw_data_directory = os.path.join(config.data_directory, "raw")
    config.processed_data_directory = os.path.join(config.data_directory, "processed")
    config.annotations_directory = os.path.join(config.data_directory, "annotations")
    config.augmented_data_directory = os.path.join(config.data_directory, "augmented")
    config.processing_config.batch_size = TEST_BATCH_SIZE
    config.processing_config.max_workers = TEST_MAX_WORKERS
    config.logging_config.log_level = TEST_LOG_LEVEL
    return config


def create_test_directories(config: ApplicationConfig) -> None:
    """
    í…ŒìŠ¤íŠ¸ìš© ë””ë ‰í„°ë¦¬ ìƒì„±

    Args:
        config: ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ê°ì²´
    """
    directories = [
        config.data_directory,
        config.raw_data_directory,
        config.processed_data_directory,
        config.annotations_directory,
        config.augmented_data_directory,
    ]

    for directory in directories:
        os.makedirs(directory, exist_ok=True)


def cleanup_test_directories(config: ApplicationConfig) -> None:
    """
    í…ŒìŠ¤íŠ¸ìš© ë””ë ‰í„°ë¦¬ ì •ë¦¬

    Args:
        config: ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ê°ì²´
    """
    if os.path.exists(config.data_directory):
        shutil.rmtree(config.data_directory)


def create_test_logger(name: str = "test_logger") -> logging.Logger:
    """
    í…ŒìŠ¤íŠ¸ìš© ë¡œê±° ìƒì„±

    Args:
        name: ë¡œê±° ì´ë¦„

    Returns:
        logging.Logger: í…ŒìŠ¤íŠ¸ìš© ë¡œê±° ê°ì²´
    """
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)

    # ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€
    if not logger.handlers:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.DEBUG)
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

    return logger


# ====================================================================================
# 4. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± í•¨ìˆ˜ë“¤
# ====================================================================================


def create_test_pdf_file(file_path: str) -> None:
    """
    í…ŒìŠ¤íŠ¸ìš© PDF íŒŒì¼ ìƒì„±

    Args:
        file_path: ìƒì„±í•  PDF íŒŒì¼ ê²½ë¡œ
    """
    # ê°„ë‹¨í•œ PDF í—¤ë” ìƒì„±
    pdf_content = b"""%PDF-1.4
1 0 obj
<< /Type /Catalog /Pages 2 0 R >>
endobj
2 0 obj
<< /Type /Pages /Kids [3 0 R] /Count 1 >>
endobj
3 0 obj
<< /Type /Page /Parent 2 0 R /MediaBox [0 0 612 792] >>
endobj
xref
0 4
0000000000 65535 f 
0000000009 00000 n 
0000000058 00000 n 
0000000115 00000 n 
trailer
<< /Size 4 /Root 1 0 R >>
startxref
190
%%EOF"""

    with open(file_path, "wb") as f:
        f.write(pdf_content)


def create_test_image_file(file_path: str, width: int = 100, height: int = 100) -> None:
    """
    í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ íŒŒì¼ ìƒì„±

    Args:
        file_path: ìƒì„±í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        width: ì´ë¯¸ì§€ ë„ˆë¹„
        height: ì´ë¯¸ì§€ ë†’ì´
    """
    try:
        from PIL import Image

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        image = Image.new("RGB", (width, height), color="white")
        image.save(file_path)
    except ImportError:
        # PILì´ ì—†ëŠ” ê²½ìš° ë”ë¯¸ ì´ë¯¸ì§€ íŒŒì¼ ìƒì„±
        dummy_image_data = b"\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00d\x00\x00\x00d\x08\x06\x00\x00\x00p\xe2\x95!..."
        with open(file_path, "wb") as f:
            f.write(dummy_image_data)


def create_test_json_file(
    file_path: str, data: Optional[Dict[str, Any]] = None
) -> None:
    """
    í…ŒìŠ¤íŠ¸ìš© JSON íŒŒì¼ ìƒì„±

    Args:
        file_path: ìƒì„±í•  JSON íŒŒì¼ ê²½ë¡œ
        data: JSON ë°ì´í„° (Noneì¸ ê²½ìš° ê¸°ë³¸ ë°ì´í„° ì‚¬ìš©)
    """
    if data is None:
        data = {
            "test_id": str(uuid.uuid4()),
            "created_at": datetime.now().isoformat(),
            "test_type": "unit_test",
            "test_data": {"documents": [], "annotations": []},
        }

    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def create_test_document_model(file_path: Optional[str] = None) -> DocumentModel:
    """
    í…ŒìŠ¤íŠ¸ìš© DocumentModel ìƒì„±

    Args:
        file_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ

    Returns:
        DocumentModel: í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ëª¨ë¸
    """
    if file_path is None:
        file_path = create_temp_test_file("test_doc.pdf")
        create_test_pdf_file(file_path)

    return DocumentModel.from_file_path(file_path)


def create_test_annotation_model(document_id: str = "test_doc") -> AnnotationModel:
    """
    í…ŒìŠ¤íŠ¸ìš© AnnotationModel ìƒì„±

    Args:
        document_id: ë¬¸ì„œ ID

    Returns:
        AnnotationModel: í…ŒìŠ¤íŠ¸ìš© ì–´ë…¸í…Œì´ì…˜ ëª¨ë¸
    """
    bbox = BoundingBox(x=10, y=20, width=100, height=50)
    annotation = AnnotationModel(
        document_id=document_id, page_number=1, annotation_type="text"
    )
    return annotation


def create_test_bounding_box(
    x: int = 10, y: int = 20, width: int = 100, height: int = 50
) -> BoundingBox:
    """
    í…ŒìŠ¤íŠ¸ìš© BoundingBox ìƒì„±

    Args:
        x: X ì¢Œí‘œ
        y: Y ì¢Œí‘œ
        width: ë„ˆë¹„
        height: ë†’ì´

    Returns:
        BoundingBox: í…ŒìŠ¤íŠ¸ìš© ë°”ìš´ë”© ë°•ìŠ¤
    """
    return BoundingBox(x=x, y=y, width=width, height=height)


def create_temp_test_file(filename: str) -> str:
    """
    ì„ì‹œ í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ ìƒì„±

    Args:
        filename: íŒŒì¼ëª…

    Returns:
        str: ì„ì‹œ íŒŒì¼ ê²½ë¡œ
    """
    temp_dir = tempfile.mkdtemp(prefix="yokogawa_test_")
    return os.path.join(temp_dir, filename)


# ====================================================================================
# 5. í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²€ì¦ í•¨ìˆ˜ë“¤
# ====================================================================================


def validate_test_environment() -> bool:
    """
    í…ŒìŠ¤íŠ¸ í™˜ê²½ ê²€ì¦

    Returns:
        bool: ê²€ì¦ ê²°ê³¼
    """
    try:
        # í•„ìˆ˜ ëª¨ë“ˆ import í™•ì¸
        import numpy as np
        import PIL

        # ì„ì‹œ ë””ë ‰í„°ë¦¬ ìƒì„± í™•ì¸
        temp_dir = tempfile.mkdtemp()
        os.rmdir(temp_dir)

        # ë¡œê¹… ì‹œìŠ¤í…œ í™•ì¸
        test_logger = create_test_logger()
        test_logger.info("Test environment validation")

        return True
    except Exception as e:
        print(f"Test environment validation failed: {e}")
        return False


def validate_test_data_integrity(data: Any) -> bool:
    """
    í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦

    Args:
        data: ê²€ì¦í•  ë°ì´í„°

    Returns:
        bool: ê²€ì¦ ê²°ê³¼
    """
    try:
        if isinstance(data, dict):
            return all(isinstance(key, str) for key in data.keys())
        elif isinstance(data, list):
            return len(data) >= 0
        elif isinstance(data, str):
            return len(data) >= 0
        else:
            return True
    except Exception:
        return False


def compare_test_results(expected: Any, actual: Any, tolerance: float = 0.001) -> bool:
    """
    í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¹„êµ

    Args:
        expected: ì˜ˆìƒ ê²°ê³¼
        actual: ì‹¤ì œ ê²°ê³¼
        tolerance: í—ˆìš© ì˜¤ì°¨ (ìˆ«ì ë¹„êµ ì‹œ)

    Returns:
        bool: ë¹„êµ ê²°ê³¼
    """
    try:
        if isinstance(expected, (int, float)) and isinstance(actual, (int, float)):
            return abs(expected - actual) <= tolerance
        elif isinstance(expected, str) and isinstance(actual, str):
            return expected == actual
        elif isinstance(expected, list) and isinstance(actual, list):
            return len(expected) == len(actual) and all(
                compare_test_results(e, a, tolerance) for e, a in zip(expected, actual)
            )
        elif isinstance(expected, dict) and isinstance(actual, dict):
            return expected.keys() == actual.keys() and all(
                compare_test_results(expected[k], actual[k], tolerance)
                for k in expected.keys()
            )
        else:
            return expected == actual
    except Exception:
        return False


# ====================================================================================
# 6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ====================================================================================


def run_test_suite(test_class: type, verbosity: int = 2) -> unittest.TestResult:
    """
    í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰

    Args:
        test_class: í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤
        verbosity: ì¶œë ¥ ìƒì„¸ë„

    Returns:
        unittest.TestResult: í…ŒìŠ¤íŠ¸ ê²°ê³¼
    """
    suite = unittest.TestLoader().loadTestsFromTestCase(test_class)
    runner = unittest.TextTestRunner(verbosity=verbosity)
    return runner.run(suite)


def measure_test_execution_time(test_func: Callable) -> tuple:
    """
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •

    Args:
        test_func: í…ŒìŠ¤íŠ¸ í•¨ìˆ˜

    Returns:
        tuple: (ì‹¤í–‰ ì‹œê°„, í…ŒìŠ¤íŠ¸ ê²°ê³¼)
    """
    import time

    start_time = time.time()
    try:
        result = test_func()
        success = True
    except Exception as e:
        result = e
        success = False
    end_time = time.time()

    execution_time = end_time - start_time
    return execution_time, result, success


def create_test_report(test_results: Dict[str, Any], output_path: str) -> None:
    """
    í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±

    Args:
        test_results: í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„°
        output_path: ë¦¬í¬íŠ¸ ì¶œë ¥ ê²½ë¡œ
    """
    report_data = {
        "test_report": {
            "timestamp": datetime.now().isoformat(),
            "environment": TEST_ENVIRONMENT,
            "results": test_results,
            "summary": {
                "total_tests": test_results.get("total_tests", 0),
                "passed_tests": test_results.get("passed_tests", 0),
                "failed_tests": test_results.get("failed_tests", 0),
                "coverage": test_results.get("coverage", 0.0),
            },
        }
    }

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)


# ====================================================================================
# 7. ëª© ê°ì²´ ìƒì„± í•¨ìˆ˜ë“¤
# ====================================================================================


def create_mock_config() -> Mock:
    """
    Mock ApplicationConfig ìƒì„±

    Returns:
        Mock: Mock ì„¤ì • ê°ì²´
    """
    mock_config = Mock(spec=ApplicationConfig)
    mock_config.environment = TEST_ENVIRONMENT
    mock_config.debug_mode = True
    mock_config.data_directory = tempfile.mkdtemp()
    mock_config.processing_config.batch_size = TEST_BATCH_SIZE
    mock_config.processing_config.max_workers = TEST_MAX_WORKERS
    return mock_config


def create_mock_logger() -> Mock:
    """
    Mock Logger ìƒì„±

    Returns:
        Mock: Mock ë¡œê±° ê°ì²´
    """
    mock_logger = Mock(spec=logging.Logger)
    mock_logger.debug = Mock()
    mock_logger.info = Mock()
    mock_logger.warning = Mock()
    mock_logger.error = Mock()
    mock_logger.critical = Mock()
    return mock_logger


def create_mock_file_handler() -> Mock:
    """
    Mock FileHandler ìƒì„±

    Returns:
        Mock: Mock íŒŒì¼ í•¸ë“¤ëŸ¬ ê°ì²´
    """
    mock_handler = Mock(spec=FileHandler)
    mock_handler.create_directory_if_not_exists = Mock(return_value=True)
    mock_handler.copy_file_with_backup = Mock(return_value=True)
    mock_handler.get_file_size_mb = Mock(return_value=10.5)
    mock_handler.calculate_file_hash = Mock(return_value="abc123")
    return mock_handler


def create_mock_service(service_type: str) -> Mock:
    """
    Mock ì„œë¹„ìŠ¤ ìƒì„±

    Args:
        service_type: ì„œë¹„ìŠ¤ íƒ€ì… ('data_collection', 'labeling', 'augmentation', 'validation')

    Returns:
        Mock: Mock ì„œë¹„ìŠ¤ ê°ì²´
    """
    mock_service = Mock()
    mock_service.initialize = Mock(return_value=True)
    mock_service.cleanup = Mock()
    mock_service.health_check = Mock(return_value=True)

    if service_type == "data_collection":
        mock_service.collect_files = Mock(return_value=["test1.pdf", "test2.pdf"])
        mock_service.get_collection_statistics = Mock(return_value={"total_files": 2})
    elif service_type == "labeling":
        mock_service.create_labeling_session = Mock(return_value="session_123")
        mock_service.get_labeling_progress = Mock(return_value={"progress": 0.5})
    elif service_type == "augmentation":
        mock_service.augment_dataset = Mock(return_value=[{"data": "augmented"}])
        mock_service.get_augmentation_statistics = Mock(
            return_value={"augmentation_factor": 3}
        )
    elif service_type == "validation":
        mock_service.validate_dataset = Mock(return_value={"validation_passed": True})
        mock_service.get_validation_statistics = Mock(
            return_value={"quality_score": 0.95}
        )

    return mock_service


# ====================================================================================
# 8. í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì¸¡ì • í•¨ìˆ˜ë“¤
# ====================================================================================


def measure_memory_usage(test_func: Callable) -> Dict[str, float]:
    """
    í…ŒìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •

    Args:
        test_func: í…ŒìŠ¤íŠ¸ í•¨ìˆ˜

    Returns:
        Dict[str, float]: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´
    """
    try:
        import psutil
        import os

        process = psutil.Process(os.getpid())

        # ì‹œì‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_func()

        # ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        final_memory = process.memory_info().rss / 1024 / 1024  # MB

        return {
            "initial_memory_mb": initial_memory,
            "final_memory_mb": final_memory,
            "memory_increase_mb": final_memory - initial_memory,
        }
    except ImportError:
        return {"error": "psutil not available"}
    except Exception as e:
        return {"error": str(e)}


def measure_cpu_usage(test_func: Callable) -> Dict[str, float]:
    """
    í…ŒìŠ¤íŠ¸ CPU ì‚¬ìš©ëŸ‰ ì¸¡ì •

    Args:
        test_func: í…ŒìŠ¤íŠ¸ í•¨ìˆ˜

    Returns:
        Dict[str, float]: CPU ì‚¬ìš©ëŸ‰ ì •ë³´
    """
    try:
        import psutil
        import time

        # CPU ì‚¬ìš©ë¥  ì¸¡ì • ì‹œì‘
        psutil.cpu_percent(interval=None)
        start_time = time.time()

        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_func()

        # ì¸¡ì • ì™„ë£Œ
        end_time = time.time()
        cpu_usage = psutil.cpu_percent(interval=None)

        return {
            "execution_time_seconds": end_time - start_time,
            "cpu_usage_percent": cpu_usage,
        }
    except ImportError:
        return {"error": "psutil not available"}
    except Exception as e:
        return {"error": str(e)}


# ====================================================================================
# 9. í…ŒìŠ¤íŠ¸ ë””ë²„ê¹… í•¨ìˆ˜ë“¤
# ====================================================================================


def debug_test_failure(
    test_name: str, exception: Exception, context: Dict[str, Any]
) -> None:
    """
    í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥

    Args:
        test_name: í…ŒìŠ¤íŠ¸ ì´ë¦„
        exception: ë°œìƒí•œ ì˜ˆì™¸
        context: í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸
    """
    print(f"\n{'='*60}")
    print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ë””ë²„ê¹… ì •ë³´: {test_name}")
    print(f"{'='*60}")
    print(f"ì˜ˆì™¸ íƒ€ì…: {type(exception).__name__}")
    print(f"ì˜ˆì™¸ ë©”ì‹œì§€: {str(exception)}")
    print(f"í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸:")
    for key, value in context.items():
        print(f"  {key}: {value}")

    # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
    import traceback

    print(f"\nìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
    traceback.print_exc()
    print(f"{'='*60}\n")


def capture_test_output(test_func: Callable) -> tuple:
    """
    í…ŒìŠ¤íŠ¸ ì¶œë ¥ ìº¡ì²˜

    Args:
        test_func: í…ŒìŠ¤íŠ¸ í•¨ìˆ˜

    Returns:
        tuple: (stdout, stderr, í…ŒìŠ¤íŠ¸ ê²°ê³¼)
    """
    import io
    import sys

    # ì¶œë ¥ ìº¡ì²˜ìš© ë²„í¼
    stdout_buffer = io.StringIO()
    stderr_buffer = io.StringIO()

    # ê¸°ì¡´ ì¶œë ¥ ë°±ì—…
    original_stdout = sys.stdout
    original_stderr = sys.stderr

    try:
        # ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰íŠ¸
        sys.stdout = stdout_buffer
        sys.stderr = stderr_buffer

        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = test_func()

        # ì¶œë ¥ ë‚´ìš© ë°˜í™˜
        return stdout_buffer.getvalue(), stderr_buffer.getvalue(), result

    finally:
        # ì›ë˜ ì¶œë ¥ ë³µì›
        sys.stdout = original_stdout
        sys.stderr = original_stderr


# ====================================================================================
# 10. í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€ ì´ˆê¸°í™” í•¨ìˆ˜
# ====================================================================================


def initialize_test_package() -> bool:
    """
    í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€ ì´ˆê¸°í™”

    Returns:
        bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
    """
    try:
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ê²€ì¦
        if not validate_test_environment():
            print("âš ï¸ í…ŒìŠ¤íŠ¸ í™˜ê²½ ê²€ì¦ ì‹¤íŒ¨")
            return False

        # í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ë””ë ‰í„°ë¦¬ ìƒì„±
        os.makedirs(TEST_REPORT_DIR, exist_ok=True)

        # í…ŒìŠ¤íŠ¸ ë¡œê±° ì„¤ì •
        test_logger = create_test_logger("test_package")
        test_logger.info("í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€ ì´ˆê¸°í™” ì™„ë£Œ")

        return True
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False


# ====================================================================================
# 11. ê³µê°œ API ì •ì˜
# ====================================================================================

__all__ = [
    # ë©”íƒ€ë°ì´í„°
    "__version__",
    "__author__",
    "__email__",
    "__description__",
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    "TEST_ENVIRONMENT",
    "TEST_LOG_LEVEL",
    "TEST_TIMEOUT_SECONDS",
    "TEST_BATCH_SIZE",
    "TEST_MAX_WORKERS",
    # í…ŒìŠ¤íŠ¸ í—¬í¼ í•¨ìˆ˜
    "create_test_config",
    "create_test_directories",
    "cleanup_test_directories",
    "create_test_logger",
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    "create_test_pdf_file",
    "create_test_image_file",
    "create_test_json_file",
    "create_test_document_model",
    "create_test_annotation_model",
    "create_test_bounding_box",
    "create_temp_test_file",
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²€ì¦
    "validate_test_environment",
    "validate_test_data_integrity",
    "compare_test_results",
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìœ í‹¸ë¦¬í‹°
    "run_test_suite",
    "measure_test_execution_time",
    "create_test_report",
    # ëª© ê°ì²´ ìƒì„±
    "create_mock_config",
    "create_mock_logger",
    "create_mock_file_handler",
    "create_mock_service",
    # ì„±ëŠ¥ ì¸¡ì •
    "measure_memory_usage",
    "measure_cpu_usage",
    # ë””ë²„ê¹…
    "debug_test_failure",
    "capture_test_output",
    # ì´ˆê¸°í™”
    "initialize_test_package",
]

# ====================================================================================
# 12. íŒ¨í‚¤ì§€ ìë™ ì´ˆê¸°í™”
# ====================================================================================

# íŒ¨í‚¤ì§€ ë¡œë“œ ì‹œ ìë™ ì´ˆê¸°í™”
_initialization_success = initialize_test_package()

if _initialization_success:
    print("âœ… YOKOGAWA OCR í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€ ì´ˆê¸°í™” ì™„ë£Œ")
else:
    print("âŒ YOKOGAWA OCR í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€ ì´ˆê¸°í™” ì‹¤íŒ¨")

# ê°œë°œ í™˜ê²½ì—ì„œë§Œ íŒ¨í‚¤ì§€ ì •ë³´ ì¶œë ¥
if os.getenv("YOKOGAWA_OCR_DEBUG", "false").lower() == "true":
    print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€ v{__version__} ë¡œë“œë¨")
    print(f"ğŸ”§ í…ŒìŠ¤íŠ¸ í™˜ê²½: {TEST_ENVIRONMENT}")
    print(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ í—¬í¼ í•¨ìˆ˜: {len(__all__)}ê°œ")
