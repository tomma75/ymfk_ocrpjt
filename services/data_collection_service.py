#!/usr/bin/env python3

"""
YOKOGAWA OCR ë°ì´í„° ì¤€ë¹„ í”„ë¡œì íŠ¸ - ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ PDF ë° ì´ë¯¸ì§€ íŒŒì¼ì„ ìˆ˜ì§‘í•˜ê³  ê²€ì¦í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì§€ì›ë˜ëŠ” í˜•ì‹ì˜ íŒŒì¼ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

ì‘ì„±ì: YOKOGAWA OCR ê°œë°œíŒ€
ì‘ì„±ì¼: 2025-07-18
"""

import os
import uuid
from pathlib import Path
from typing import Any, Dict, List, Optional, Callable, Set, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from core.base_classes import BaseService, DataCollectionInterface
from core.exceptions import (
    DataCollectionError,
    FileAccessError,
    FileFormatError,
    ProcessingError,
    ValidationError,
)
from config.settings import ApplicationConfig
from config.constants import (
    SUPPORTED_FILE_FORMATS,
    PDF_FILE_EXTENSIONS,
    IMAGE_FILE_EXTENSIONS,
    MAX_FILE_SIZE_MB,
    DEFAULT_BATCH_SIZE,
    FILE_PROCESSING_TIMEOUT_SECONDS,
)
from models.document_model import DocumentModel, DocumentType
from utils.file_handler import FileHandler
from utils.logger_util import get_application_logger


class FileCollector:
    """
    íŒŒì¼ ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤

    ì§€ì •ëœ ê²½ë¡œì—ì„œ ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ì˜ íŒŒì¼ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """

    def __init__(self, supported_formats: List[str]):
        """
        FileCollector ì´ˆê¸°í™”

        Args:
            supported_formats: ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ ëª©ë¡
        """
        self.supported_formats = supported_formats
        self.logger = get_application_logger("file_collector")

    def collect_files_from_directory(
        self, source_path: str, recursive: bool = True
    ) -> List[str]:
        """
        ë””ë ‰í„°ë¦¬ì—ì„œ íŒŒì¼ ìˆ˜ì§‘

        Args:
            source_path: ìˆ˜ì§‘í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ
            recursive: í•˜ìœ„ ë””ë ‰í„°ë¦¬ í¬í•¨ ì—¬ë¶€

        Returns:
            List[str]: ìˆ˜ì§‘ëœ íŒŒì¼ ê²½ë¡œ ëª©ë¡

        Raises:
            FileAccessError: ë””ë ‰í„°ë¦¬ ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ
        """
        try:
            if not os.path.exists(source_path):
                raise FileAccessError(
                    message=f"Source directory not found: {source_path}",
                    file_path=source_path,
                    access_type="read",
                )

            collected_files = []
            source_path_obj = Path(source_path)

            if recursive:
                pattern = "**/*"
                file_paths = source_path_obj.glob(pattern)
            else:
                pattern = "*"
                file_paths = source_path_obj.glob(pattern)

            for file_path in file_paths:
                if file_path.is_file():
                    file_extension = file_path.suffix.lower()
                    if file_extension in self.supported_formats:
                        collected_files.append(str(file_path.absolute()))

            self.logger.info(
                f"Collected {len(collected_files)} files from {source_path}"
            )
            return collected_files

        except Exception as e:
            self.logger.error(f"Failed to collect files from {source_path}: {str(e)}")
            raise DataCollectionError(
                message=f"File collection failed: {str(e)}",
                source_path=source_path,
                original_exception=e,
            )


class MetadataExtractor:
    """
    ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° í´ë˜ìŠ¤

    ìˆ˜ì§‘ëœ íŒŒì¼ë“¤ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """

    def __init__(self, file_handler: FileHandler):
        """
        MetadataExtractor ì´ˆê¸°í™”

        Args:
            file_handler: íŒŒì¼ ì²˜ë¦¬ í•¸ë“¤ëŸ¬
        """
        self.file_handler = file_handler
        self.logger = get_application_logger("metadata_extractor")

    def extract_file_metadata(self, file_path: str) -> Dict[str, Any]:
        """
        íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

        Args:
            file_path: íŒŒì¼ ê²½ë¡œ

        Returns:
            Dict[str, Any]: ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°

        Raises:
            ProcessingError: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        try:
            metadata = self.file_handler.get_file_metadata(file_path)

            # íŒŒì¼ íƒ€ì…ë³„ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            file_extension = Path(file_path).suffix.lower()

            if file_extension in PDF_FILE_EXTENSIONS:
                metadata["document_type"] = DocumentType.PDF.value
            elif file_extension in IMAGE_FILE_EXTENSIONS:
                metadata["document_type"] = DocumentType.IMAGE.value
            else:
                metadata["document_type"] = DocumentType.UNKNOWN.value

            # ì¶”ê°€ í’ˆì§ˆ ì •ë³´
            metadata["is_valid"] = self._validate_file_quality(file_path, metadata)
            metadata["extraction_timestamp"] = datetime.now().isoformat()

            return metadata

        except Exception as e:
            self.logger.error(f"Failed to extract metadata from {file_path}: {str(e)}")
            raise ProcessingError(
                message=f"Metadata extraction failed: {str(e)}",
                processor_id="metadata_extractor",
                processing_stage="metadata_extraction",
                original_exception=e,
            )

    def _validate_file_quality(self, file_path: str, metadata: Dict[str, Any]) -> bool:
        """
        íŒŒì¼ í’ˆì§ˆ ê²€ì¦

        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            metadata: ë©”íƒ€ë°ì´í„°

        Returns:
            bool: í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
        """
        try:
            # íŒŒì¼ í¬ê¸° ê²€ì¦
            if metadata.get("file_size_mb", 0) > MAX_FILE_SIZE_MB:
                return False

            # íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦
            return self.file_handler.validate_file_integrity(file_path)

        except Exception as e:
            self.logger.warning(
                f"File quality validation failed for {file_path}: {str(e)}"
            )
            return False


class DuplicateDetector:
    """
    ì¤‘ë³µ íŒŒì¼ íƒì§€ê¸° í´ë˜ìŠ¤

    ìˆ˜ì§‘ëœ íŒŒì¼ë“¤ ì¤‘ ì¤‘ë³µ íŒŒì¼ì„ íƒì§€í•©ë‹ˆë‹¤.
    """

    def __init__(self, file_handler: FileHandler):
        """
        DuplicateDetector ì´ˆê¸°í™”

        Args:
            file_handler: íŒŒì¼ ì²˜ë¦¬ í•¸ë“¤ëŸ¬
        """
        self.file_handler = file_handler
        self.logger = get_application_logger("duplicate_detector")
        self._hash_cache: Dict[str, str] = {}

    def detect_duplicates(self, file_list: List[str]) -> List[str]:
        """
        ì¤‘ë³µ íŒŒì¼ íƒì§€

        Args:
            file_list: ê²€ì‚¬í•  íŒŒì¼ ëª©ë¡

        Returns:
            List[str]: ê³ ìœ í•œ íŒŒì¼ ëª©ë¡ (ì¤‘ë³µ ì œê±°ë¨)
        """
        try:
            unique_files = []
            file_hashes: Dict[str, str] = {}

            for file_path in file_list:
                try:
                    # íŒŒì¼ í•´ì‹œ ê³„ì‚°
                    file_hash = self._get_file_hash(file_path)

                    if file_hash not in file_hashes:
                        file_hashes[file_hash] = file_path
                        unique_files.append(file_path)
                    else:
                        self.logger.info(
                            f"Duplicate file detected: {file_path} (original: {file_hashes[file_hash]})"
                        )

                except Exception as e:
                    self.logger.warning(f"Failed to process file {file_path}: {str(e)}")
                    continue

            duplicate_count = len(file_list) - len(unique_files)
            self.logger.info(
                f"Removed {duplicate_count} duplicate files from {len(file_list)} total files"
            )

            return unique_files

        except Exception as e:
            self.logger.error(f"Duplicate detection failed: {str(e)}")
            raise ProcessingError(
                message=f"Duplicate detection failed: {str(e)}",
                processor_id="duplicate_detector",
                processing_stage="duplicate_detection",
                original_exception=e,
            )

    def _get_file_hash(self, file_path: str) -> str:
        """
        íŒŒì¼ í•´ì‹œ ì¡°íšŒ (ìºì‹œ í™œìš©)

        Args:
            file_path: íŒŒì¼ ê²½ë¡œ

        Returns:
            str: íŒŒì¼ í•´ì‹œ
        """
        if file_path in self._hash_cache:
            return self._hash_cache[file_path]

        file_hash = self.file_handler.calculate_file_hash(file_path)
        self._hash_cache[file_path] = file_hash
        return file_hash


class DataCollectionService(BaseService, DataCollectionInterface):
    """
    ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤

    íŒŒì¼ ìˆ˜ì§‘, ë©”íƒ€ë°ì´í„° ì¶”ì¶œ, ì¤‘ë³µ ì œê±° ë“±ì˜ ë°ì´í„° ìˆ˜ì§‘ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    BaseServiceì™€ DataCollectionInterfaceë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
    """

    def __init__(self, config: ApplicationConfig, logger):
        """
        DataCollectionService ì´ˆê¸°í™”

        Args:
            config: ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ê°ì²´
            logger: ë¡œê±° ê°ì²´
        """
        super().__init__(config, logger)

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.file_handler = FileHandler(config)
        self.file_collector = FileCollector(SUPPORTED_FILE_FORMATS)
        self.metadata_extractor = MetadataExtractor(self.file_handler)
        self.duplicate_detector = DuplicateDetector(self.file_handler)

        # ì„¤ì • ì •ë³´
        self.batch_size = config.processing_config.batch_size
        self.max_workers = config.processing_config.max_workers
        self.processing_timeout = FILE_PROCESSING_TIMEOUT_SECONDS

        # ìƒíƒœ ê´€ë¦¬
        self.collected_files: List[str] = []
        self.collected_documents: List[DocumentModel] = []
        self.collection_statistics: Dict[str, Any] = {}
        self.collection_callbacks: List[Callable] = []

        # ì§„í–‰ ìƒíƒœ
        self.collection_progress: float = 0.0
        self.current_operation: Optional[str] = None
        self.processing_errors: List[str] = []

        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½
        self._lock = threading.Lock()

    def initialize(self) -> bool:
        """
        ì„œë¹„ìŠ¤ ì´ˆê¸°í™”

        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.logger.info("Initializing DataCollectionService")

            # ìƒíƒœ ì´ˆê¸°í™”
            with self._lock:
                self.collected_files.clear()
                self.collected_documents.clear()
                self.collection_statistics.clear()
                self.collection_callbacks.clear()
                self.processing_errors.clear()
                self.collection_progress = 0.0
                self.current_operation = None

            # íŒŒì¼ í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ê²€ì¦
            if not self.file_handler:
                raise ProcessingError("FileHandler initialization failed")

            self.logger.info("DataCollectionService initialized successfully")
            return True

        except Exception as e:
            self.logger.error(f"Failed to initialize DataCollectionService: {str(e)}")
            self._is_initialized = False
            return False

    def cleanup(self) -> None:
        """
        ì„œë¹„ìŠ¤ ì •ë¦¬
        """
        try:
            self.logger.info("Cleaning up DataCollectionService")

            with self._lock:
                self.collected_files.clear()
                self.collected_documents.clear()
                self.collection_statistics.clear()
                self.collection_callbacks.clear()
                self.processing_errors.clear()

            # íŒŒì¼ í•¸ë“¤ëŸ¬ ì •ë¦¬
            if hasattr(self.file_handler, "cleanup_temp_files"):
                self.file_handler.cleanup_temp_files()

            self.logger.info("DataCollectionService cleanup completed")

        except Exception as e:
            self.logger.error(f"Error during DataCollectionService cleanup: {str(e)}")
    
    def health_check(self) -> bool:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            # ì´ˆê¸°í™” ìƒíƒœ í™•ì¸
            if not self.is_initialized():
                self.logger.warning("Service not initialized")
                return False

            # ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ í™•ì¸
            if not hasattr(self, "config") or self.config is None:
                self.logger.warning("Config is None")
                return False

            return True

        except Exception as e:
            self.logger.error(f"Health check failed: {str(e)}")
            return False

    def collect_files(self, source_path: str) -> List[str]:
        """
        íŒŒì¼ ìˆ˜ì§‘ (DataCollectionInterface êµ¬í˜„)

        Args:
            source_path: ìˆ˜ì§‘í•  ê²½ë¡œ

        Returns:
            List[str]: ìˆ˜ì§‘ëœ íŒŒì¼ ê²½ë¡œ ëª©ë¡
        """
        try:
            self.logger.info(f"Starting file collection from: {source_path}")

            with self._lock:
                self.current_operation = "file_collection"
                self.collection_progress = 0.0

            # íŒŒì¼ ìˆ˜ì§‘
            self._update_progress(0.1, "Collecting files from directory")
            raw_files = self.file_collector.collect_files_from_directory(
                source_path, recursive=True
            )
            
            # PDF íŒŒì¼ì„ í˜ì´ì§€ë³„ë¡œ ë¶„í• 
            self._update_progress(0.2, "Processing PDF files")
            processed_files = self._process_pdf_files(raw_files)

            # ì¤‘ë³µ ì œê±°
            self._update_progress(0.3, "Removing duplicate files")
            unique_files = self.duplicate_detector.detect_duplicates(processed_files)

            # íŒŒì¼ ê²€ì¦
            self._update_progress(0.5, "Validating file integrity")
            validated_files = self._validate_files_batch(unique_files)

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ë¬¸ì„œ ëª¨ë¸ ìƒì„±
            self._update_progress(
                0.7, "Extracting metadata and creating document models"
            )
            document_models = self._create_document_models(validated_files)

            # ê²°ê³¼ ì €ì¥
            with self._lock:
                self.collected_files = validated_files
                self.collected_documents = document_models
                self.collection_progress = 1.0
                self.current_operation = None

            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_collection_statistics()

            # ì½œë°± ì‹¤í–‰
            self._execute_collection_callbacks()

            self.logger.info(
                f"File collection completed: {len(validated_files)} files collected"
            )
            return validated_files

        except Exception as e:
            self.logger.error(f"File collection failed: {str(e)}")
            with self._lock:
                self.processing_errors.append(str(e))
                self.current_operation = None
            raise DataCollectionError(
                message=f"File collection failed: {str(e)}",
                source_path=source_path,
                original_exception=e,
            )

    def collect_pdf_files(self, source_path: str) -> List[str]:
        """
        PDF íŒŒì¼ ìˆ˜ì§‘

        Args:
            source_path: ìˆ˜ì§‘í•  ê²½ë¡œ

        Returns:
            List[str]: ìˆ˜ì§‘ëœ PDF íŒŒì¼ ê²½ë¡œ ëª©ë¡
        """
        try:
            all_files = self.collect_files(source_path)
            pdf_files = [
                f for f in all_files if Path(f).suffix.lower() in PDF_FILE_EXTENSIONS
            ]

            self.logger.info(f"Collected {len(pdf_files)} PDF files from {source_path}")
            return pdf_files

        except Exception as e:
            self.logger.error(f"PDF file collection failed: {str(e)}")
            raise DataCollectionError(
                message=f"PDF file collection failed: {str(e)}",
                source_path=source_path,
                original_exception=e,
            )

    def get_collection_statistics(self) -> Dict[str, Any]:
        """
        ìˆ˜ì§‘ í†µê³„ ì •ë³´ ì œê³µ (DataCollectionInterface êµ¬í˜„)

        Returns:
            Dict[str, Any]: ìˆ˜ì§‘ í†µê³„ ì •ë³´
        """
        with self._lock:
            return self.collection_statistics.copy()

    def register_collection_callback(self, callback: Callable) -> None:
        """
        ìˆ˜ì§‘ ì™„ë£Œ ì‹œ ì½œë°± ë“±ë¡ (DataCollectionInterface êµ¬í˜„)

        Args:
            callback: ì½œë°± í•¨ìˆ˜
        """
        with self._lock:
            self.collection_callbacks.append(callback)

        self.logger.debug(f"Collection callback registered: {callback.__name__}")

    def categorize_files_by_type(self, file_list: List[str]) -> Dict[str, List[str]]:
        """
        íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜

        Args:
            file_list: ë¶„ë¥˜í•  íŒŒì¼ ëª©ë¡

        Returns:
            Dict[str, List[str]]: íƒ€ì…ë³„ ë¶„ë¥˜ëœ íŒŒì¼ ëª©ë¡
        """
        try:
            categorized_files = {"pdf": [], "image": [], "unknown": []}

            for file_path in file_list:
                file_extension = Path(file_path).suffix.lower()

                if file_extension in PDF_FILE_EXTENSIONS:
                    categorized_files["pdf"].append(file_path)
                elif file_extension in IMAGE_FILE_EXTENSIONS:
                    categorized_files["image"].append(file_path)
                else:
                    categorized_files["unknown"].append(file_path)

            self.logger.info(
                f"Files categorized: PDF={len(categorized_files['pdf'])}, "
                f"Image={len(categorized_files['image'])}, "
                f"Unknown={len(categorized_files['unknown'])}"
            )

            return categorized_files

        except Exception as e:
            self.logger.error(f"File categorization failed: {str(e)}")
            raise ProcessingError(
                message=f"File categorization failed: {str(e)}",
                processor_id=self.service_id,
                processing_stage="file_categorization",
                original_exception=e,
            )

    def validate_file_integrity(self, file_path: str) -> bool:
        """
        íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦

        Args:
            file_path: ê²€ì¦í•  íŒŒì¼ ê²½ë¡œ

        Returns:
            bool: ë¬´ê²°ì„± ê²€ì¦ ê²°ê³¼
        """
        try:
            return self.file_handler.validate_file_integrity(file_path)

        except Exception as e:
            self.logger.error(
                f"File integrity validation failed for {file_path}: {str(e)}"
            )
            return False

    def extract_file_metadata(self, file_path: str) -> Dict[str, Any]:
        """
        íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

        Args:
            file_path: íŒŒì¼ ê²½ë¡œ

        Returns:
            Dict[str, Any]: ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°
        """
        try:
            return self.metadata_extractor.extract_file_metadata(file_path)

        except Exception as e:
            self.logger.error(f"Metadata extraction failed for {file_path}: {str(e)}")
            raise ProcessingError(
                message=f"Metadata extraction failed: {str(e)}",
                processor_id=self.service_id,
                processing_stage="metadata_extraction",
                original_exception=e,
            )

    def detect_duplicates(self, file_list: List[str]) -> List[str]:
        """
        ì¤‘ë³µ íŒŒì¼ íƒì§€

        Args:
            file_list: ê²€ì‚¬í•  íŒŒì¼ ëª©ë¡

        Returns:
            List[str]: ê³ ìœ í•œ íŒŒì¼ ëª©ë¡
        """
        try:
            return self.duplicate_detector.detect_duplicates(file_list)

        except Exception as e:
            self.logger.error(f"Duplicate detection failed: {str(e)}")
            raise ProcessingError(
                message=f"Duplicate detection failed: {str(e)}",
                processor_id=self.service_id,
                processing_stage="duplicate_detection",
                original_exception=e,
            )

    def get_collected_documents(self) -> List[DocumentModel]:
        """
        ìˆ˜ì§‘ëœ ë¬¸ì„œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜

        Returns:
            List[DocumentModel]: ìˆ˜ì§‘ëœ ë¬¸ì„œ ëª¨ë¸ ëª©ë¡
        """
        with self._lock:
            return self.collected_documents.copy()

    def get_collection_progress(self) -> Dict[str, Any]:
        """
        ìˆ˜ì§‘ ì§„í–‰ ìƒí™© ë°˜í™˜

        Returns:
            Dict[str, Any]: ì§„í–‰ ìƒí™© ì •ë³´
        """
        with self._lock:
            return {
                "progress": self.collection_progress,
                "current_operation": self.current_operation,
                "collected_files_count": len(self.collected_files),
                "collected_documents_count": len(self.collected_documents),
                "processing_errors_count": len(self.processing_errors),
            }

    def _validate_files_batch(self, file_list: List[str]) -> List[str]:
        """
        íŒŒì¼ ë°°ì¹˜ ê²€ì¦

        Args:
            file_list: ê²€ì¦í•  íŒŒì¼ ëª©ë¡

        Returns:
            List[str]: ê²€ì¦ í†µê³¼ íŒŒì¼ ëª©ë¡
        """
        validated_files = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_file = {
                executor.submit(self.validate_file_integrity, file_path): file_path
                for file_path in file_list
            }

            for future in as_completed(future_to_file, timeout=self.processing_timeout):
                file_path = future_to_file[future]
                try:
                    is_valid = future.result()
                    if is_valid:
                        validated_files.append(file_path)
                    else:
                        self.logger.warning(f"File validation failed: {file_path}")
                except Exception as e:
                    self.logger.error(f"Error validating file {file_path}: {str(e)}")

        return validated_files

    def _create_document_models(self, file_list: List[str]) -> List[DocumentModel]:
        """
        ë¬¸ì„œ ëª¨ë¸ ìƒì„±

        Args:
            file_list: íŒŒì¼ ëª©ë¡

        Returns:
            List[DocumentModel]: ìƒì„±ëœ ë¬¸ì„œ ëª¨ë¸ ëª©ë¡
        """
        document_models = []

        for file_path in file_list:
            try:
                document_model = DocumentModel.from_file_path(file_path)
                document_models.append(document_model)
            except Exception as e:
                self.logger.error(
                    f"Failed to create document model for {file_path}: {str(e)}"
                )
                with self._lock:
                    self.processing_errors.append(
                        f"Document model creation failed for {file_path}: {str(e)}"
                    )

        return document_models
    
    def _process_pdf_files(self, file_list: List[str]) -> List[str]:
        """
        PDF íŒŒì¼ì„ í˜ì´ì§€ë³„ë¡œ ë¶„í• í•˜ì—¬ ì €ì¥
        
        Args:
            file_list: íŒŒì¼ ëª©ë¡
            
        Returns:
            List[str]: ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡
        """
        processed_files = []
        
        # ë¨¼ì € ì „ì²´ í˜ì´ì§€ ìˆ˜ ê³„ì‚°
        total_pages = 0
        pdf_files_info = []
        
        for file_path in file_list:
            if Path(file_path).suffix.lower() == '.pdf':
                try:
                    from PyPDF2 import PdfReader
                    with open(file_path, 'rb') as pdf_file:
                        pdf_reader = PdfReader(pdf_file)
                        page_count = len(pdf_reader.pages)
                        pdf_files_info.append((file_path, page_count))
                        total_pages += page_count
                        self.logger.info(f"PDF {Path(file_path).name} has {page_count} pages")
                except Exception as e:
                    self.logger.warning(f"Could not count pages for {file_path}: {str(e)}")
                    pdf_files_info.append((file_path, 0))
            else:
                # PDFê°€ ì•„ë‹Œ íŒŒì¼ì€ ê·¸ëŒ€ë¡œ ì¶”ê°€
                processed_files.append(file_path)
        
        if total_pages > 0:
            self.logger.info(f"Total PDF pages to process: {total_pages}")
        
        # PDF íŒŒì¼ì„ í˜ì´ì§€ë³„ë¡œ ë³€í™˜í•˜ë©´ì„œ í”„ë¡œê·¸ë ˆìŠ¤ ì—…ë°ì´íŠ¸
        processed_pages = 0
        
        for file_path, expected_pages in pdf_files_info:
            try:
                # PDFë¥¼ í˜ì´ì§€ë³„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                from pdf2image import convert_from_path
                import tempfile
                
                output_dir = Path(self.config.processed_data_directory) / 'images'
                output_dir.mkdir(parents=True, exist_ok=True)
                
                # PDF íŒŒì¼ëª… (extension ì œì™¸)
                pdf_name = Path(file_path).stem
                
                # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                images = convert_from_path(file_path, dpi=300)
                
                # ê° í˜ì´ì§€ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
                for i, image in enumerate(images, 1):
                    page_filename = f"{pdf_name}_page_{i:03d}.png"
                    page_path = output_dir / page_filename
                    image.save(str(page_path), 'PNG')
                    processed_files.append(str(page_path))
                    
                    # í”„ë¡œê·¸ë ˆìŠ¤ ì—…ë°ì´íŠ¸
                    processed_pages += 1
                    if total_pages > 0:
                        progress = 0.2 + (0.1 * processed_pages / total_pages)  # 0.2 ~ 0.3 êµ¬ê°„
                        self._update_progress(
                            progress, 
                            f"Converting PDF pages to PNG: {processed_pages}/{total_pages}"
                        )
                    
                    self.logger.info(f"Saved PDF page {i}/{len(images)} to: {page_path}")
                
            except Exception as e:
                self.logger.error(f"Failed to process PDF {file_path}: {str(e)}")
                # PDF ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ íŒŒì¼ ì¶”ê°€
                processed_files.append(file_path)
        
        return processed_files

    def _update_progress(self, progress: float, operation: str) -> None:
        """
        ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸

        Args:
            progress: ì§„í–‰ë¥  (0.0 ~ 1.0)
            operation: í˜„ì¬ ì‘ì—… ì„¤ëª…
        """
        with self._lock:
            self.collection_progress = progress
            self.current_operation = operation

        self.logger.debug(f"Progress updated: {progress:.1%} - {operation}")

    def _update_collection_statistics(self) -> None:
        """
        ìˆ˜ì§‘ í†µê³„ ì—…ë°ì´íŠ¸
        """
        with self._lock:
            file_types = self.categorize_files_by_type(self.collected_files)

            self.collection_statistics = {
                "total_files_collected": len(self.collected_files),
                "total_documents_created": len(self.collected_documents),
                "file_types": {
                    "pdf_count": len(file_types["pdf"]),
                    "image_count": len(file_types["image"]),
                    "unknown_count": len(file_types["unknown"]),
                },
                "processing_errors_count": len(self.processing_errors),
                "collection_timestamp": datetime.now().isoformat(),
                "service_id": self.service_id,
            }

    def _execute_collection_callbacks(self) -> None:
        """
        ìˆ˜ì§‘ ì™„ë£Œ ì½œë°± ì‹¤í–‰
        """
        with self._lock:
            callbacks = self.collection_callbacks.copy()

        for callback in callbacks:
            try:
                callback(self.collected_documents)
            except Exception as e:
                self.logger.error(f"Collection callback execution failed: {str(e)}")

    @classmethod
    def create_with_dependencies(cls, container) -> "DataCollectionService":
        """
        ì˜ì¡´ì„± ì»¨í…Œì´ë„ˆë¥¼ ì‚¬ìš©í•œ íŒ©í† ë¦¬ ë©”ì„œë“œ

        Args:
            container: ì˜ì¡´ì„± ì»¨í…Œì´ë„ˆ

        Returns:
            DataCollectionService: ìƒì„±ëœ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        """
        return cls(
            config=container.get_service("config"),
            logger=container.get_service("logger"),
        )


# ëª¨ë“ˆ ìˆ˜ì¤€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_data_collection_service(config: ApplicationConfig) -> DataCollectionService:
    """
    ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ ìƒì„± í•¨ìˆ˜

    Args:
        config: ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •

    Returns:
        DataCollectionService: ìƒì„±ëœ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
    """
    logger = get_application_logger("data_collection_service")
    service = DataCollectionService(config, logger)

    if not service.initialize():
        raise ProcessingError("Failed to initialize DataCollectionService")

    return service


if __name__ == "__main__":
    # ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸
    print("YOKOGAWA OCR ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    try:
        # ì„¤ì • ë¡œë“œ
        from config.settings import load_configuration

        config = load_configuration()

        # ì„œë¹„ìŠ¤ ìƒì„±
        service = create_data_collection_service(config)

        # ìƒíƒœ í™•ì¸
        if service.health_check():
            print("âœ… ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ ì •ìƒ ë™ì‘")
        else:
            print("âŒ ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ ìƒíƒœ ì´ìƒ")

        # í†µê³„ ì •ë³´ ì¶œë ¥
        statistics = service.get_collection_statistics()
        print(f"ğŸ“Š ìˆ˜ì§‘ í†µê³„: {statistics}")

        # ì •ë¦¬
        service.cleanup()

    except Exception as e:
        print(f"âŒ ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    print("\nğŸ¯ ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ êµ¬í˜„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
